---
title: "Working_code"
author: "Douglas Hannum"
date: "3/30/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(ReIns)
library(survival)
library(ggfortify)
library(svMisc)
library(survminer)
library(GGally)
```

Design Question 2
Cancers of the blood, such as leukemia and multiple myeloma, that are unresponsive to other forms of treatment, can be treated with an allogeneic hematopoietic stem cell transplant (alloHSCT), in which the stem cells come from a genetically-matched donor, usually a sibling.

However, the percentage of matching is often imperfect, leading to the immune system (T cells) in the alloHSCT (graft) and the remaining immune system in the recipient (host) to fight each other. Acute graft- versus-host disease (aGVHD) is the name of this immune reaction if it occurs within 100 days of alloHSCT. Acute GVHD occurs in 40-50% of alloHSCT recipients, and the skin, gut, and liver are the organ systems most affected. Acute GVHD is one of the leading causes of early death in HSCT recipients and few approaches are available to either prevent or treat aGVHD. The most common approach for treatment is heavy-dose steroids, which present patients with other serious side effects even if the aGVHD is eliminated.

The process of alloHSCT has been shown to result in alteration of the intestinal microbiome, the bacterial community that lives in the human intestine that is believed to be central to gut health. Thus, it is hoped that treatments that directly work to maintain the intestinal microbiome will thereby lead to a reduced incidence of aGVHD in alloHSCT patients.

It is hypothesized that increased consumption of potato starch will work to increase or maintain levels of butyrate, which is a short-chain fatty acid that plays numerous roles in the intestinal microbiome. You are asked to help design a one-arm clinical trial, in which all enrolled alloHSCT patients will ingest 40g of starch each day for up to 100 days in hopes of preventing aGVHD from developing.

Information pertinent to the design: historically, the median time-to-onset of aGVHD is 28 days, with an interquartile range of [7, 56] days.

1. Historically, the median time-to-onset of aGVHD is 28 days, with an interquartile range of [7, 56] days.

1. Approximately 15% of patients will experience graft failure, relapse of cancer, or death by Day 100 and will not be observable for aGVHD. Assume that dietary starch has no impact on any of these outcomes.

1. Approximately 20% of patients will discontinue taking starch for a variety of reasons unrelated to aGVHD.

Specific questions to address:

1. How many patients need to be enrolled to reduce the 50% historical rate of aGVHD to each of 40%,35%, and 30%? Assume power of 0.80 and a Type I error rate of 0.05.

1. It is possible that the study should be terminated early because the treatment is appearing to be ineffective. Assume that a rate of 50% would be evidence of ineffectiveness. How would you design the study to include one interim analysis of futility, and how does this affect the sample size you found in (1) if no interim analysis were used?


##Creating the distribution 

```{r distribution}
#Going to use an exponential distribution for the survival function. Truncated at 100

#CDF 1 - exp(-lambda*x) / (1 - exp(-100*lambda))

#Median = 28 with IQR of (7, 56)
#Solving for lambda

# median = ln(2)*(lambda)^ - 1

lambda <- (28/log(2))**(-1)
lambda

#.91588127 of aGVHD occur within the 100 day truncation

dist <- function(x){
    rexp(x, lambda)   
}
x <- NA
y <- NA
z <- NA
for (i in 1:100){
        x[i] <- median(dist(2000))
        y[i] <- quantile(dist(2000), probs = .25)
        z[i] <- quantile(dist(2000), probs = .75)
}
#Close to the distribution I want but the 25% is still to high
w <- NA
for (i in 1:50000){
        w[i] <- sum(dist(2000) > 100)/ 2000
}

#This is the percentage of events we will miss with a max of 100 days follow-up

summary(w)
quantile(w, c(.025,.975))

# Point estimate for the percentage of events we will miss is .0840 (.0720, 0.0965)

#Sample Size
ss <- 30:130
samples <- ss[1]

x <- runif(samples)
f <- function(x,u){
        return(1-exp(-lambda*x) - u);
}

z <- rep(NA, samples)

z <- log(1-x)/(-lambda)
```

```{r another way}
#example
n <- 40
samples <- 100
test <- rep(NA, samples)

for (i in 1:samples){
       x <- rbinom(n, 1, .5)
       y <- rbinom(n, 1, .4)
       test[i] <- t.test(x,y)$p.value
}

samples <- 1000
n <- seq(40,210, by = 5)
permutations <- 1000

power <-as.data.frame(matrix(ncol = permutations + 2, nrow = length(n)*3))
colnames(power) <- c('Sample Size','Delta', paste0('Power_',1:permutations))
power[,2] <- rep(c('0.10','0.15','0.20'), each = length(n))

power[,1] <- rep(n,3)
pvalue <- rep(NA, samples)
probs <- c(.40,.35,.30)

#Don't want this to accidentally run again

# for (p in 1:permutations){
#         for (k in 1:length(probs)){
#                 for (i in 1:length(n)){
#                         for (j in 1:samples){
#                                 x <- rbinom(n[i], 1, .5)
#                                 y <- rbinom(n[i], 1, probs[k])
#                                 pvalue[j] <- t.test(x,y)$p.value
#                         }
#                         power[i +(k-1)*length(n),2 +p] <- sum(pvalue < 0.05)/(samples)
#                 }
#         }
# }
power_samples <- power[,3:1002]
power$power_mean <- rowMeans(power[,3:1002])

power$power_lower <- NA
power$power_upper <- NA
for (i in 1:dim(power)[1]){
        power$power_lower[i] <- quantile(power[i,3:1002], .025)
        power$power_upper[i] <- quantile(power[i,3:1002], .975)
}

ggplot(data = power, aes(x = `Sample Size`, y = power_mean, colour = Delta)) +
        geom_point() + geom_line() + 
        geom_errorbar(aes(x = `Sample Size`,ymin = power_lower, ymax = power_upper))
```

```{r back to first way}
lambda <- seq(.02,.03, by = .0001)
mtx <- as.data.frame(matrix(ncol = 4, nrow = length(lambda)))
colnames(mtx) <- c('lambda','1st qu.','Median','3rd qu.')
mtx$lambda <- lambda

for (i in 1:length(lambda)){
        sum <- summary(rtexp(1000,lambda[i], endpoint = 100))
        mtx[i,2:4] <- sum[c(2,3,5)]
}

#Solved lambda for the median theoretically
lambda <- 0.0203821

summary(rtexp(10000,lambda, endpoint = 100))
```

#Building the Simulation

```{r setting up data frame}
#Gives me a number of random observations (x) from the truncated exponential dist
rdist <- function(x = 1){
        rtexp(x, lambda, endpoint = 100)
}

#A function to create the data frames, n is the sample size, p is the 
#disease prevalence

dataf <- function(n,p){
        df <- as.data.frame(matrix(ncol = 2, nrow = n, rep(c(200,0), each = n)))
        colnames(df) <- c('Time','Event')
        
        df$Event <- rbinom(n,1,p)
        df[df$Event == 1,]$Time <- rdist(sum(df$Event))
        
        return (df)
}

#Seeing if the survival curve contains 0.50 in its CI at time = 100

surv_sig <- function (df){
        fit <- survfit(Surv(Time, Event) ~1, data = df)
        l <- summary(fit, times = 100)$lower
        return(l > 0.50)
}
```

```{r putting it together}
sample_power <- function(p, samples = 1000, l = 40 , u = 210 , by = 10, perm = 1){
        n <- seq(l,u, by = by)
        
        data <- as.data.frame(matrix(ncol = 1+perm, nrow = length(n), 
                                     c(n, rep(0,length(n)*perm))))
        
        colnames(data) <- c('Sample Size',paste0('Power',1:perm))
        
        
        for(k in 1:perm){
                for (i in 1:length(n)){
                        progress(i)
                        vect <- rep(NA,samples)
                        for (j in 1:samples){
                                df <- dataf(n[i],p)
                                vect[j] <- surv_sig (df)
                        }
                        data[i,1+k] <- round(mean(vect),4)
                }
        }
        return (data)
}


for (i in 1:length(n)){
        vect <- rep(NA,samples)
        for (j in 1:samples){
                df <- dataf(n[i],p)
                        vect[j] <- surv_sig (df)
        }
        data[i,2] <- mean(vect)
}
```

```{r demo}
start <- Sys.time()
power.40 <- sample_power(.4)
power.30 <- sample_power(.3)
power.35 <- sample_power(.35)
end <- Sys.time()

power <- rbind(power.40, power.30)
power <- rbind(power, power.35)
power$Rate <- rep(c('0.40','0.30','0.35'), each = dim(power.40)[1])

ggplot(data = power, aes(x = `Sample Size`, y = Power1, colour = Rate)) +
        geom_point() + geom_line() + ylab ('Power') +
        geom_hline(yintercept = .8, linetype = 2)
#ggsave('./images/surv1', device = 'png', height = 4, width = 6, units = 'in')
```

```{r demo w/ more permutations}
# start <- Sys.time()
# power.40 <- sample_power(.4, perm = 50)
# power.30 <- sample_power(.3, perm = 50)
# power.35 <- sample_power(.35, perm = 50)
# end <- Sys.time()

end - start

power50 <- rbind(power.40, power.30)
power50 <- rbind(power50, power.35)
power50$rate <- rep(c('0.40','0.30','0.35'), each = dim(power.40)[1])
permutations <- power50[,grepl('Power', colnames(power50))]
permutations$mean <- rowMeans(permutations)
permutations$sd <- NA

for (i in 1:dim(permutations)[1]){
        permutations$sd[i] <- sd(permutations[i,])
}
permutations$upper <- permutations$mean - 1.96*permutations$sd
permutations$lower <- permutations$mean + 1.96*permutations$sd

permutations$upper2 <- NA
permutations$lower2 <- NA

for (i in 1:dim(permutations)[1]){
        permutations$upper2[i] <- as.numeric(quantile(permutations[i,1:50], .975))
        permutations$lower2[i] <- as.numeric(quantile(permutations[i,1:50], .025))
}

power50_ <- cbind(power50, permutations[,51:56])


ggplot(data = power50_, aes(x = `Sample Size`, y = mean, color = rate)) +
        #geom_errorbar(aes(x = `Sample Size`, ymin = as.numeric(lower2), ymax = as.numeric(upper2))) +
        geom_ribbon(aes(ymin = lower2, ymax = upper2), alpha = 0.3) +
        geom_point() + geom_line() + geom_hline(yintercept = .8, 
                                                linetype = 'dashed',
                                                color = 'black') +
        ylab('Mean Power')
        
#ggsave('./images/sample2', device = 'png', height = 4, width = 6, units = 'in')        
```

```{r}
#Only one permutation but with a larger sample and more sample sizes
one <- sample_power(p = 0.40, by = 2, l = 35)
two <- sample_power(p = 0.35, by = 2, l = 35)
three <- sample_power(p = 0.30, by = 2, l = 35)

trial3 <- rbind(one,two)
trial3 <- rbind(trial3, three)
trial3$Rate <- rep(c('0.40','0.35','0.30'), each = dim(one)[1])

ggplot(data = trial3, aes (x = `Sample Size`, y = Power1, colour = Rate)) + 
        geom_point() + geom_line() + geom_smooth() +
        geom_hline(yintercept = .8, linetype =2) +
        ylab ('Power')

#ggsave('./images/sample3', device = 'png', height = 4, width = 6, units = 'in')
```

```{r binomiallyl view}
samples <- 1000
n <- seq(40,210, by = 5)
permutations <- 100

power <-as.data.frame(matrix(ncol = permutations + 2, nrow = length(n)*3))
colnames(power) <- c('Sample Size','Rate', paste0('Power_',1:permutations))
power[,2] <- rep(c('0.40','0.35','0.30'), each = length(n))

power[,1] <- rep(n,3)
pvalue <- rep(NA, samples)
probs <- c(.40,.35,.30)

power[,1:2] <- lapply(power[,1:2], as.numeric)

for (i in 1:dim(power)[1]){
        progress(i)
        for (k in 1:permutations){
                
                for (j in 1:samples){
                        x <- rbinom(power[i,1], 1, .5)
                        y <- rbinom(power[i,1], 1, 1-power[i,2])
                        pvalue[j] <- t.test(x,y)$p.value
                        
                }
                power[i,k+2] <- sum(pvalue < 0.05)/samples
        }
}

bpower <- power
bpower$Power <- rowMeans(bpower[,3:103])

bpower$lower <- NA
bpower$upper <- NA

for (i in 1:dim(bpower)[1]){
        bpower$lower[i] <- quantile(bpower[i,2:52], .025)
        bpower$upper[i] <- quantile(bpower[i,2:52], .975)
}


ggplot(data = bpower, aes (x = `Sample Size`, y = Power, colour = as.factor(Rate))) + geom_point() +
        geom_ribbon(aes(ymin = as.numeric(lower), ymax = as.numeric(upper)), 
                    fill = 'grey70', alpha = 0.3) +
        geom_line() + 
        ylab('Power')
        
```

This seems really low

```{r binom practice}
x <- rbinom(100,1,.5)
y <- rbinom(100,1,.4)
t.test(x,y)$p.value

bpower[100:105,100:105]
```

```{r truncated exponential function}
w <- rdist(1000)
summary(w)
wd <- as.data.frame(matrix(ncol = 2, nrow = 1000, c(w, 1:1000)))
wd <- wd[order(wd$V1),]

ggplot(data = wd, aes(x = V1)) + 
        geom_density() + ylab ('Density') + xlab ('Time to Event') +
        ggtitle('Truncated Exponential Distribution') +
        theme(plot.title = element_text(hjust = 0.5)) +
        theme_bw()
#ggsave('./images/tr_exp_distr.png', device = 'png', units = 'in',
#       width = 6, height = 4)
```

```{r survival plot for ppt}
df1 <- dataf(80, .35)
df1$Event[1] <- 1

fit1 <- survfit(Surv(Time, Event) ~1, data = df1)
ss <- subset(surv_summary(fit1), time < 100)
ggsurv <- ggsurvplot (ss, main = "One Simulation (n = 80, rate = 0.35)",
                      conf.int = T, risk.table = T, risk.table.col = 'strata',
                      ggtheme = theme_bw())
ggsurv

# ggsave('./images/smple_km_plot.png', device = 'png', units = 'in', 
#        height = 3, width = 6)
```

```{r random truncated normal distribution}
library(truncnorm)
rdistn <- function(x = 1){
        rtruncnorm(x, a = 0, b = 100, mean = 50, sd = 25)
}

wn <- rdistn(1000)
summary(wn)
wnd <- as.data.frame(matrix(ncol = 2, nrow = 1000, c(wn, 1:1000)))
wnd <- wnd[order(wnd$V1),]

ggplot(data = wnd, aes(x = V1)) + 
        geom_density() + ylab ('Density') + xlab ('Time to Event') +
        ggtitle('Truncated Normal Distribution') +
        theme(plot.title = element_text(hjust = 0.5)) +
        theme_bw()

# ggsave('./images/tr_norm_distr.png', device = 'png', units = 'in',
#       width = 6, height = 4)
summary(wn)
```

